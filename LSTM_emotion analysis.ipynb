{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GoEmotion Dataset Sentiment Analysis\n",
    "\n",
    "This assignment is used for Spring 2020 CSCI-360 machine learning final project.\n",
    "The sentiment analysis with multiple emotion labels will be mainly performed by LSTM network using keras with pretrained tokenizers.\n",
    "Baseline model was built and performed beforehand(naive bayes, decision tree, mlp, cnn)\n",
    "\n",
    "Author: Kaiyan Zhan(kz2271),"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import the dataset and perform data-preprocessing. In the original dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "dir = '/Users/zhankaiyan/PycharmProjects/CSCI-360/final_project/archive 3/data/full_dataset/'\n",
    "set1 = pd.read_csv(dir + 'goemotions_1.csv')\n",
    "set2 = pd.read_csv(dir + 'goemotions_2.csv')\n",
    "set3 = pd.read_csv(dir + 'goemotions_3.csv')\n",
    "\n",
    "data = pd.concat([set1, set2, set3], ignore_index=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "with open('/Users/zhankaiyan/PycharmProjects/CSCI-360/final_project/archive 3/data/ekman_mapping.json') as file:\n",
    "    ekman_mapping = json.load(file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': ['anger', 'annoyance', 'disapproval'], 'disgust': ['disgust'], 'fear': ['fear', 'nervousness'], 'joy': ['joy', 'amusement', 'approval', 'excitement', 'gratitude', 'love', 'optimism', 'relief', 'pride', 'admiration', 'desire', 'caring'], 'sadness': ['sadness', 'disappointment', 'embarrassment', 'grief', 'remorse'], 'surprise': ['surprise', 'realization', 'confusion', 'curiosity']}\n"
     ]
    }
   ],
   "source": [
    "print(ekman_mapping)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "take a look at the data. The one-hot key is already embedded in the original dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(211225, 37)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
      "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.548084e+09        37                  True           0  ...     0   \n",
      "2  1.546428e+09        37                 False           0  ...     0   \n",
      "3  1.547965e+09        18                 False           0  ...     1   \n",
      "4  1.546669e+09         2                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "5  Right? Considering it’s such an important docu...  eespn2i   \n",
      "\n",
      "   example_very_unclear  admiration  amusement  anger  annoyance  approval  \\\n",
      "0                 False           0          0      0          0         0   \n",
      "2                 False           0          0      0          0         0   \n",
      "3                 False           0          0      0          0         0   \n",
      "4                 False           0          0      0          0         0   \n",
      "5                 False           0          0      0          0         0   \n",
      "\n",
      "   caring  confusion  ...  love  nervousness  optimism  pride  realization  \\\n",
      "0       0          0  ...     0            0         0      0            0   \n",
      "2       0          0  ...     0            0         0      0            0   \n",
      "3       0          0  ...     1            0         0      0            0   \n",
      "4       0          0  ...     0            0         0      0            0   \n",
      "5       0          0  ...     0            0         0      0            0   \n",
      "\n",
      "   relief  remorse  sadness  surprise  neutral  \n",
      "0       0        0        1         0        0  \n",
      "2       0        0        0         0        1  \n",
      "3       0        0        0         0        0  \n",
      "4       0        0        0         0        1  \n",
      "5       0        0        0         0        0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop the unclear data and only keep the necessary column\n",
    "data = data[~data['example_very_unclear']]\n",
    "data = data.drop(columns=['author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id'])\n",
    "# data = data.drop(columns=['neutral'])\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "5  Right? Considering it’s such an important docu...  eespn2i   \n",
      "\n",
      "   example_very_unclear  admiration  amusement  anger  annoyance  approval  \\\n",
      "0                 False           0          0      0          0         0   \n",
      "2                 False           0          0      0          0         0   \n",
      "3                 False           0          0      0          0         0   \n",
      "4                 False           0          0      0          0         0   \n",
      "5                 False           0          0      0          0         0   \n",
      "\n",
      "   caring  confusion  ...  love  nervousness  optimism  pride  realization  \\\n",
      "0       0          0  ...     0            0         0      0            0   \n",
      "2       0          0  ...     0            0         0      0            0   \n",
      "3       0          0  ...     1            0         0      0            0   \n",
      "4       0          0  ...     0            0         0      0            0   \n",
      "5       0          0  ...     0            0         0      0            0   \n",
      "\n",
      "   relief  remorse  sadness  surprise  neutral  \n",
      "0       0        0        1         0        0  \n",
      "2       0        0        0         0        1  \n",
      "3       0        0        0         0        0  \n",
      "4       0        0        0         0        1  \n",
      "5       0        0        0         0        0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(207814, 2)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping emotions:\n",
    "anger_list = [ \"anger\", \"annoyance\", \"disapproval\", \"disgust\"]\n",
    "fear_list = [\"fear\", \"nervousness\"]\n",
    "joy_list = [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",\"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"]\n",
    "sadness_list = [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\", \"remorse\"]\n",
    "surprise_list = [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n",
    "emotion_groups = [anger_list, fear_list, joy_list, sadness_list, surprise_list]\n",
    "\n",
    "\"\"\"\n",
    "Labels:\n",
    "Anger (0) : [“Anger”, “annoyance”, “disapproval”, “disgust”]\n",
    "Fear (1) : [“fear”, “nervousness” ]\n",
    "Joy (2) : [“joy” , “amusement”, “approval”, “excitement”, “gratitude”,\n",
    "     “love”, “optimism”, “relief”, “pride”, “admiration”, “desire”, “caring”]\n",
    "Sadness (3) : [“Sadness”, “Disappointment”, “Embarrassment”, “grief”, “remorse”]\n",
    "Surprise (4) : [“Surprise”, “Realization”, “confusion”, “curiosity”]\n",
    "Neutral (5) : [\"Neutral\"]\n",
    "\"\"\"\n",
    "col_names = ['text','group_label']\n",
    "new_data = []\n",
    "for id,row in data.iterrows():\n",
    "    if row['example_very_unclear'] == True:\n",
    "        continue\n",
    "    else:\n",
    "        if row['neutral'] == True:\n",
    "            info = [row['text'], 5]\n",
    "        else:\n",
    "            max_cnt = -1\n",
    "            max_label = -1\n",
    "            for ix,eg in enumerate(emotion_groups):\n",
    "                cnt = 0\n",
    "                for label in eg:\n",
    "                    if row[label] == 1:\n",
    "                        cnt += 1\n",
    "                if cnt > max_cnt:\n",
    "                    max_cnt = cnt\n",
    "                    max_label = ix\n",
    "            info = [row['text'], max_label]\n",
    "        new_data.append(info)\n",
    "\n",
    "emotion_group = pd.DataFrame(np.array(new_data),columns=col_names)\n",
    "emotion_group.head()\n",
    "emotion_group.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "the data in the dataset was highly imbalanced(extremely little data in 'fear'). Reorganise to have a balanced class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "2    79279\n5    55298\n0    33937\n4    20967\n3    14292\n1     4041\nName: group_label, dtype: int64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_group.group_label.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "num_of_text = 4000\n",
    "shuffled = emotion_group.reindex(np.random.permutation(emotion_group.index))\n",
    "anger = shuffled[shuffled['group_label'] == '0'][:num_of_text]\n",
    "fear = shuffled[shuffled['group_label'] == '1'][:num_of_text]\n",
    "joy = shuffled[shuffled['group_label'] == '2'][:num_of_text]\n",
    "sad = shuffled[shuffled['group_label'] == '3'][:num_of_text]\n",
    "surprise = shuffled[shuffled['group_label'] == '4'][:num_of_text]\n",
    "neutral = shuffled[shuffled['group_label'] == '5'][:num_of_text]\n",
    "concated = pd.concat([anger,fear,joy,sad, surprise, neutral], ignore_index=True)\n",
    "\n",
    "concated = concated.reindex(np.random.permutation(concated.index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text group_label\n",
      "7922   He’s probably just getting stressed out that i...           1\n",
      "10832  I agree. More stupid ideas by ''the adults in ...           2\n",
      "13046  leave bro- this is the exact type of behavior ...           3\n",
      "21817      You were. I gave you away to see Tool in '06.           5\n",
      "6735   Certain mental health issues can be smelled on...           1\n",
      "...                                                  ...         ...\n",
      "10357  Unfortunately it isn’t available in my locatio...           2\n",
      "6513   [NAME] \" haters gonna hate\" the most cringe I'...           1\n",
      "4645   Blizzard was working with Activision long befo...           1\n",
      "21093          Sorry, [NAME], but [NAME] beat you to it.           5\n",
      "4992   Aaaand I'm still here. I don't have a alt acco...           1\n",
      "\n",
      "[24000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(concated)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "perform one-hot key encoding again"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7922     1\n",
      "10832    2\n",
      "13046    3\n",
      "21817    5\n",
      "6735     1\n",
      "11988    2\n",
      "21310    5\n",
      "15181    3\n",
      "14300    3\n",
      "17873    4\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "concated['LABEL'] = 0\n",
    "concated.loc[concated['group_label'] == '0', 'LABEL'] = 0\n",
    "concated.loc[concated['group_label'] == '1', 'LABEL'] = 1\n",
    "concated.loc[concated['group_label'] == '2', 'LABEL'] = 2\n",
    "concated.loc[concated['group_label'] == '3', 'LABEL'] = 3\n",
    "concated.loc[concated['group_label'] == '4', 'LABEL'] = 4\n",
    "concated.loc[concated['group_label'] == '5', 'LABEL'] = 5\n",
    "print(concated['LABEL'][:10])\n",
    "labels = to_categorical(concated['LABEL'], num_classes=6)\n",
    "print(labels[:10])\n",
    "if 'group_label' in concated.keys():\n",
    "    concated.drop(['group_label'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenizing with text filtering, perform text cleaning using regular expressions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18285 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_most_common_words = 8000\n",
    "max_len = 130\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(concated['text'].values)\n",
    "sequences = tokenizer.texts_to_sequences(concated['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# X = keras.pad_sequences(sequences, maxlen=max_len)\n",
    "X = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## model setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)\n",
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining hyperparameter\n",
    "epochs = 10\n",
    "emb_dim = 128\n",
    "batch_size = 256\n",
    "labels[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 130, 128)          1024000   \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, 130, 128)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,073,798\n",
      "Trainable params: 1,073,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 4218s 74s/step - loss: 1.7850 - acc: 0.2008 - val_loss: 1.7818 - val_acc: 0.2783\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 5218s 92s/step - loss: 1.7087 - acc: 0.3214 - val_loss: 1.7410 - val_acc: 0.3267\n",
      "Epoch 3/10\n",
      "36/57 [=================>............] - ETA: 26:03 - loss: 1.5600 - acc: 0.4030"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "txt = [\"\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "pred = model.predict(padded)\n",
    "labels = emotion_groups + ['neutral_list']\n",
    "for segment in labels:\n",
    "    segment = segment[:-5]\n",
    "print(pred, labels[np.argmax(pred)])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
